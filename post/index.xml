<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Patrick Saux</title>
    <link>https://sauxpa.github.io/post/</link>
      <atom:link href="https://sauxpa.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-uk</language>
    <image>
      <url>https://sauxpa.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://sauxpa.github.io/post/</link>
    </image>
    
    <item>
      <title>Gambler&#39;s ruin in Astro and the accuracy of Gaussian approximation [3]</title>
      <link>https://sauxpa.github.io/post/astro_3/</link>
      <pubDate>Sat, 12 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://sauxpa.github.io/post/astro_3/</guid>
      <description>&lt;p&gt;In the previous posts, we calculated the distribution of the so-called &amp;ldquo;flight time&amp;rdquo; $\tau_{\alpha}$, interpreted as the (random) number of Astro card a player can scratch before loosing more than their initial investment $\alpha$ (typically $\alpha$ is €2), both in closed-form under a continuous time Gaussian setting and empirically using random simulations of the exact Astro loss random walk. These two approaches showed that $\frac{\alpha}{\mu}$, where $\mu$ is the average loss of random walk step, was a good estimation of $\mathbb{E}\left[ \tau_{\alpha}\right]$; in the case of Astro, this estimate is $\mathbb{E}\left[ \tau_{\alpha}\right]\approx 3.17$. We now show that this estimation is actually correct. We will actually prove a more general result, namely a closed-form expression of the Laplace transform of $\tau_{\alpha}$, based on the method of martingales already used in the Gaussian approximation.&lt;/p&gt;
&lt;p&gt;We recall that $\left(\xi_t\right)_{t\in\mathbb{N}}$ denotes a i.i.d sequence of random Astro losses and let
$$
X_t = \sum_{s=1}^t \xi_s
$$
be the cumulative loss after $t\in\mathbb{N}$ steps, and that the flight time is defined by the equation
$$
\tau_{\alpha} = \inf\left\lbrace t\in\mathbb{N}, X_t \geq \alpha\right\rbrace.
$$&lt;/p&gt;
&lt;h2 id=&#34;what-goes-wrong-in-the-method-of-martingales-for-discrete-random-walks&#34;&gt;What goes wrong in the method of martingales for discrete random walks?&lt;/h2&gt;
&lt;p&gt;In the continuous time Gaussian setting, we replaced $\tau_{\alpha}$ and $\left(X_t\right)_{t\in\mathbb{N}}$ with $\bar{\tau}_{\alpha} = \left\lbrace t\in\mathbb{R}_+, Z_t \geq \alpha\right\rbrace$ and $\left(Z_t\right)_{t\in\mathbb{R}_+}$ satisfying the stochastic differential equation $dZ_t = \mu dt + \sigma dW_t$, driven by a Brownian motion $W$. Then for any $\lambda\in\mathbb{R}$,
$$
M^\lambda_t = \exp\left( \frac{\lambda}{\sigma} Z_t - (\frac{\lambda^2}{2} + \frac{\lambda \mu}{\sigma})t\right)
$$
defines a martingale and a standard application of Doob&amp;rsquo;s optional stopping theorem showed that $\mathbb{E}\left[M^\lambda_{\bar{\tau}_{\alpha}}\right]=1$. Now in general, this gives information on two random variables simultaneously: $\bar{\tau}_{\alpha}$ and $Z_{\bar{\tau}_{\alpha}}$. The whole trick is to disentangle the contribution of each of these variables and extract the Laplace transform of $\bar{\tau}_{\alpha}$, which is $\mathbb{E}\left[e^{-\beta \bar{\tau}_{\alpha}}\right]$ for $\beta&amp;gt;0$. In the continuous time setting, the almost sure continuity of $t\mapsto Z_t$ and the definition of $\bar{\tau}_{\alpha}$ shows that $Z_{\tau_{\alpha}}=\alpha$ (almost surely), which is not random at all, thus revealing information directly on $\bar{\tau}_{\alpha}$.&lt;/p&gt;
&lt;p&gt;By contrast, the discrete time counterpart $X_{\tau_\alpha}$ does not satisfy a similar equality in general, but only the inequality $X_{\tau_\alpha} \geq \alpha$. Worse, the distribution of $X_{\tau_\alpha}$ can be quite difficult to characterise as there are potentially many different sequences of steps $\xi_1, \dots, \xi_{\tau_\alpha}$ that result in crossing the barrier $\alpha$. In a special case of step distribution however, $X_{\tau_\alpha}$ takes a simple form, namely if the process $X$ can only increase by a fixed step size, or decrease by multiples of the same step size. More precisely, let $\Delta&amp;gt;0$, $(p_k)_{k\in\mathbb{N}}$ such that $p_k\in[0, 1]$ for all $k\in\mathbb{N}$ and $\sum_{k\in\mathbb{N}} p_k &amp;lt; 1$, and define the  i.i.d. step sequence $(\xi)_{t\in\mathbb{N}}$ by
$$
\xi_t = \begin{cases}
-k\Delta&amp;amp; \text{with probability $p_k$}\,,\\&lt;br&gt;
\Delta&amp;amp; \text{with probability $1-\sum_{k\in\mathbb{N}} p_k$}\,.\\&lt;br&gt;
\end{cases}
$$
Indeed, this forces the process $X$ to move over the grid $\Delta \mathbb{Z}$, and since $\alpha&amp;gt;0$ and $X$ can only increase by $\Delta$, this implies that $X_{\tau_{\alpha}}$ can only take a single value corresponding to $X_{\tau_{\alpha}-1}&amp;lt;\alpha$ and $\xi_{\tau_{\alpha}}=\Delta$, i.e. $X_{\tau_{\alpha}}=\lceil \frac{\alpha}{\Delta}\rceil\Delta =: \alpha^+$, where $\lceil x \rceil$ denotes the ceil operator (smallest integer larger than $x$).&lt;/p&gt;
&lt;p&gt;As it turns out, this rather specific property is satisfied by the Astro step distribution: $\xi_t$ is equal to €2 minus the gain of the $t$-th ticket, and since all gains are multiple of €2, with a minimum of 0€, the above property holds with $\Delta$ equal to €2 and $(p_k)_{k\in\mathbb{N}}$ the corresponding gain probabilities.&lt;/p&gt;
&lt;h2 id=&#34;exponential-supermartingale-and-expected-flight-time&#34;&gt;Exponential supermartingale and expected flight time&lt;/h2&gt;
&lt;p&gt;Using the property that $X_{\tau_\alpha}$, the martingale method developed in the first part for the Gaussian approximation can be applied to the process $X$ essentially unchanged. Let $\xi$ denote a generic random variable following the Astro step distribution and define
$$
\psi\colon \lambda\in\mathbb{R}^*_+ \mapsto \log\mathbb{E}\left[ e^{\lambda \xi}\right]\,.
$$
Then, for any $\lambda&amp;gt;0$, the process defined for $t\in\mathbb{N}$ by $M^\lambda_t = e^{X_t  - t\psi(\lambda)}$ is a martingale. Using the expression of the step size sequence, we have
$$
\psi(\lambda) = \lambda \Delta + \log\left(1 - \sum_{k\in\mathbb{N}}p_k\left(1 - e^{-(k+1)\Delta}\right)\right)\,.
$$
Moreover, $e^{\psi(\lambda)} = \mathbb{E}\left[e^{\lambda \xi}\right] \geq e^{\lambda \mu}$ by convexity and Jensen&amp;rsquo;s inequality, where
$$
\mu=\mathbb{E}\left[\xi\right] = \Delta \left( 1 - \sum_{k\in\mathbb{N}} (1+k)p_k\right)\,.
$$
and hence $\psi(\lambda) \geq \lambda \mu &amp;gt;0$.&lt;/p&gt;
&lt;p&gt;The same arguments as for the Gaussian approximation (dominated convergence, Doob&amp;rsquo;s optional stopping theorem) apply and show that $\mathbb{E}\left[ e^{-\psi(\lambda) \tau_{\alpha}}\right] = e^{-\lambda \alpha^+}$. Noting that $\psi$ is invertible and using the change of variable $\beta = \psi(\lambda)$, we deduce that
$$
\mathbb{E}\left[ e^{-\beta \tau_{\alpha}}\right] = e^{-\psi^{-1}(\beta) \alpha^+}\,,
$$
which is the expression of the Laplace transform of $\tau_{\alpha}$. To obtain the expected flight time $\mathbb{E}\left[ \tau_{\alpha}\right]$, we differentiate with respect to $\beta$, which gives
$$
-\frac{\partial}{\partial \beta} \mathbb{E}\left[e^{-\beta \tau_{\alpha}}\right] = \mathbb{E}\left[ \tau_{\alpha}\right] = (\psi^{-1})&#39;(0)\alpha^+ e^{-\psi^{-1}(0)\alpha^+} = (\psi^{-1})&#39;(0)\alpha^+\,,
$$
since $\psi(0)=0$.&lt;/p&gt;
&lt;p&gt;The term $(\psi^{-1})&#39;(0)$ can be calculated using the inverse function rule, which yields $(\psi^{-1})&#39;(0)=\frac{1}{\psi&#39;(0)}$. A direct calculation shows that
$$
\psi&#39;(\lambda) = \Delta + \frac{-\Delta\sum_{k\in\mathbb{N}}(k+1)p_k e^{-(k+1) \Delta \lambda}}{1 - \sum_{k\in\mathbb{N}}p_k\left( 1 - e^{-(k+1)\Delta}\right)}\,,
$$
and thus $\psi&#39;(0) = \Delta\left(1 - \sum_{k\in\mathbb{N}}(k+1)p_k\right) = \mu$. Going back to the expected flight time, we have
$$
\mathbb{E}\left[ \tau_{\alpha} \right] = \frac{\alpha^+}{\mu}\,.
$$
In particular for the Astro distribution, $\alpha=\Delta=\alpha^+$; in other words, $\mathbb{E}[\tau_{\alpha}]=\mathbb{E}\left[\bar{\tau}_{\alpha}\right]$, i.e. the Gaussian approximation was actually correct in expectation!&lt;/p&gt;
&lt;h2 id=&#34;going-further-higher-moments&#34;&gt;Going further: higher moments&lt;/h2&gt;
&lt;p&gt;Thanks to the exact calculation of the Laplace transform of $\tau_{\alpha}$, it is possible to derive higher moments by differentiating $\mathbb{E}\left[e^{-\beta \tau_{\alpha}}\right]$ multiple times. A key observation is that the function $\psi$ fully describes the &lt;em&gt;cumulants&lt;/em&gt; $(\kappa_n)_{n\geq 1}$ of the distribution of $\xi$, which are an alternative to moments (intuitively, the $n$-th cumulant is the component of the $n$-th moment that is &amp;ldquo;independent&amp;rdquo; of the previous moments; for instance, $\kappa_2$ is the variance of $\xi$ rather than $\mathbb{E}[\xi^2]$). More formally, the cumulants are defined by the power series expansion
$$
\psi(\lambda) = \sum_{n=1}^{+\infty} \kappa_n \frac{\lambda^n}{n!}\,,
$$
in particular $\psi^{(n)}(0)=\kappa_n$. Therefore, by successive differentiations of $\beta\mapsto e^{-\psi^{-1}(\beta)\alpha^+}$ or $\beta\mapsto -\psi^{-1}(\beta)\alpha^+$, we can obtain the successive moments or cumulants of $\tau_{\alpha}$.&lt;/p&gt;
&lt;p&gt;For instance, applying twice the inverse function rule yields
$$
\left(\psi^{-1}\right)&#39;&#39;\left(\beta\right) = -\frac{\psi&#39;&#39;(\lambda)}{\psi&#39;(\lambda)^3}\,,
$$
and thus the variance of $\tau_{\alpha}$ is
$$
\mathbb{V}\left[\tau_{\alpha}\right] = \frac{\alpha^+ \sigma^2}{\mu^3}\,,
$$
where $\sigma\approx 20.67$ is the standard deviation of $\xi$. As it turns out, this is also exactly the formula provided by the inverse Gaussian distribution of $\bar{\tau}_{\alpha}$!&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2022-present &lt;a href=&#34;https://sauxpa.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Patrick Saux&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gambler&#39;s ruin in Astro and the accuracy of Gaussian approximation [2]</title>
      <link>https://sauxpa.github.io/post/astro_2/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://sauxpa.github.io/post/astro_2/</guid>
      <description>&lt;h2 id=&#34;assessing-the-quality-of-the-gaussian-approximation&#34;&gt;Assessing the quality of the Gaussian approximation&lt;/h2&gt;
&lt;p&gt;In the previous post, we have calculated the distribution of the flight time of the cumulative Gaussian loss process $Z_t=\mu t + \sigma W_t$, that is $\bar{\tau}_{\alpha} = \inf \left\lbrace t\in\mathbb{R}_+, Z_t \geq \alpha \right\rbrace$. In the game of Astro though, the loss distribution for scratching a card is far from Gaussian: it is discrete, asymmetrical, with pronounced skewness towards small losses. The figure below shows the difference between the true Astro distribution (in green) and its Gaussian approximation $\mathcal{N}(\mu, \sigma^2)$ (in blue), with $\mu=0.63$ and $\sigma=20.67$.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /post/astro_2/astro_vs_gaussian_distributions_hu72c0f51c5aa2e2c6c44f0c4e9b7d19e8_25858_7947c7dfcc6c3ac0105073d8031c3722.png 400w,
               /post/astro_2/astro_vs_gaussian_distributions_hu72c0f51c5aa2e2c6c44f0c4e9b7d19e8_25858_a6c83265bff505e2f4d821c3c29f1986.png 760w,
               /post/astro_2/astro_vs_gaussian_distributions_hu72c0f51c5aa2e2c6c44f0c4e9b7d19e8_25858_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/astro_2/astro_vs_gaussian_distributions_hu72c0f51c5aa2e2c6c44f0c4e9b7d19e8_25858_7947c7dfcc6c3ac0105073d8031c3722.png&#34;
               width=&#34;760&#34;
               height=&#34;339&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;It is insightful to look at the empirical estimation of $\mu$ and $\sigma$ under both the true Astro model and its Gaussian approximation. We define the standard (unbiased) sample estimators
\begin{align*}
&amp;amp;\widehat{\mu}_t = \frac{1}{t}\sum_{s=1}^t X_s \,,\\&lt;br&gt;
&amp;amp;\widehat{\sigma}^2_t = \frac{1}{t-1}\sum_{s=1}^t \left(X_s - \widehat{\mu}_t\right)^2 \,,
\end{align*}
and report below the median (25th - 75th percentile in shaded area) of 10 independent replications of $\widehat{\mu}_t$ and $\widehat{\sigma}_t$ for $t$ ranging from 2 to 4,500,000, where $X$ is sampled from the Astro distribution (in green) and the Gaussian approximation (in blue).&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /post/astro_2/mu_sigma_astro_vs_gaussian_distributions_hubeb908b7006fc2257a7621c266e289b1_79434_d70fa162b0026c6d9f7e7fe36cf9d868.png 400w,
               /post/astro_2/mu_sigma_astro_vs_gaussian_distributions_hubeb908b7006fc2257a7621c266e289b1_79434_8add3d03d48a04eb9acb971018d12892.png 760w,
               /post/astro_2/mu_sigma_astro_vs_gaussian_distributions_hubeb908b7006fc2257a7621c266e289b1_79434_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/astro_2/mu_sigma_astro_vs_gaussian_distributions_hubeb908b7006fc2257a7621c266e289b1_79434_d70fa162b0026c6d9f7e7fe36cf9d868.png&#34;
               width=&#34;496&#34;
               height=&#34;712&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;For the mean estimation, as expected, $\widehat{\mu}_t$ converges to the true expected value $\mu=0.63$, with higher dispersion in the Gaussian case. For the variance estimation, $\widehat{\sigma}_t$ seems to converge quite fast to the true value $\sigma=20.67$ in the Gaussian case, but remains much lower in the Astro model until $t\approx 10^5$, before suddenly jumping to values around $\widehat{\sigma}_t\approx 20$. This is actually quite intuitive: the high variance of the Astro distribution is driven by the outlier gains, in particular 1,000 and 25,000, which occur with very small probability. Before $t\approx 10^5$, the samples $X_1, \dots, X_t$ used in $\widehat{\sigma}_t$ do not contain such outliers, thus estimating a seemingly low variance. In other words, the sample variance in the Astro model is subject to higher variability than in the Gaussian case, which hints at larger higher order moments (skewness, kurtosis&amp;hellip;) in the Astro case. We confirm this intuition with a direct calculation, based on the formulae:&lt;/p&gt;
&lt;p&gt;\begin{align*}
&amp;amp;skewness = \frac{\mathbb{E}\left[\left(X-\mu\right)^3\right]}{\sigma^{\frac{3}{2}}}\,,\\&lt;br&gt;
&amp;amp;kurtosis = \frac{\mathbb{E}\left[\left(X-\mu\right)^4\right]}{\sigma^{2}} - 3\,.
\end{align*}&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;$\mu$&lt;/th&gt;
&lt;th&gt;$\sigma$&lt;/th&gt;
&lt;th&gt;skewness&lt;/th&gt;
&lt;th&gt;kurtosis&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Astro&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;td&gt;20.67&lt;/td&gt;
&lt;td&gt;-1180&lt;/td&gt;
&lt;td&gt;1426486&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gaussian&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;td&gt;20.67&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;simulated-flight-time&#34;&gt;Simulated flight time&lt;/h2&gt;
&lt;p&gt;Contrary to the continuous time Gaussian case $\bar{\tau}_{\alpha}$, computing $\tau_{\alpha}=\inf\left\lbrace t\in\mathbb{N}, X_t \geq \alpha\right\rbrace$ in closed-form for the true Astro distribution is a priori intricate. However, it is straightforward to simulate random walks following the Astro distribution and therefore to estimate $\tau_{\alpha}$ by the Monte Carlo method. The result of 100 millions independent replications are reported below against the inverse Gaussian distribution coming from the Gaussian approximation model.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /post/astro_2/astro_vs_gaussian_flight_time_distributions_hu7a3a15ba6dace7201dd4329569ad7e76_26354_ae283f78c9470905ef8d476be901d24c.png 400w,
               /post/astro_2/astro_vs_gaussian_flight_time_distributions_hu7a3a15ba6dace7201dd4329569ad7e76_26354_453a1166625d2185a56f7ab14bb84d34.png 760w,
               /post/astro_2/astro_vs_gaussian_flight_time_distributions_hu7a3a15ba6dace7201dd4329569ad7e76_26354_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/astro_2/astro_vs_gaussian_flight_time_distributions_hu7a3a15ba6dace7201dd4329569ad7e76_26354_ae283f78c9470905ef8d476be901d24c.png&#34;
               width=&#34;496&#34;
               height=&#34;352&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;The simulation reveals that the inverse Gaussian approximation underestimates the probability of small flight times and overestimates that of longer ($&amp;gt;100$) flight times. In other words, the inverse Gaussian approximation does not account properly for the high risk of early crash. We report mean, standard deviation and percentiles of $\tau_{\alpha}$ and $\bar{\tau}_{\alpha}$, i.e. under both the simulated Astro and the inverse Gaussian models (95% confidence intervals for the simulated statistics are calculated by bootstrap with 400 independent replications).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;$\mu$&lt;/th&gt;
&lt;th&gt;$\sigma$&lt;/th&gt;
&lt;th&gt;5th&lt;/th&gt;
&lt;th&gt;25th&lt;/th&gt;
&lt;th&gt;50th&lt;/th&gt;
&lt;th&gt;75th&lt;/th&gt;
&lt;th&gt;95th&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Astro (simulated) $\tau_{\alpha}$&lt;/td&gt;
&lt;td&gt;3.20 &lt;br /&gt;[3.15 ; 3.27]&lt;/td&gt;
&lt;td&gt;60.8 &lt;br /&gt;[39.9 ; 78.4]&lt;/td&gt;
&lt;td&gt;1.0 &lt;br /&gt;[1.0 ; 1.0]&lt;/td&gt;
&lt;td&gt;1.0 &lt;br /&gt;[1.0 ; 1.0]&lt;/td&gt;
&lt;td&gt;1.0 &lt;br /&gt;[1.0 ; 1.0]&lt;/td&gt;
&lt;td&gt;2.0 &lt;br /&gt;[2.0 ; 2.0]&lt;/td&gt;
&lt;td&gt;11.0 &lt;br /&gt;[11.0 ; 11.0]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IG approximation $\bar{\tau}_{\alpha}$&lt;/td&gt;
&lt;td&gt;3.17&lt;/td&gt;
&lt;td&gt;58.46&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;td&gt;0.02&lt;/td&gt;
&lt;td&gt;0.09&lt;/td&gt;
&lt;td&gt;2.14&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The expectation of $\tau_{\alpha}$ is, perhaps surprisingly, well estimated in the inverse Gaussian model; in fact, it is statistically plausible that $\mathbb{E}\left[\tau_{\alpha}\right] = \mathbb{E}\left[\bar{\tau}_{\alpha}\right]$ ($3.17 \in [3.15 ; 3.27]$). This is actually somewhat intuitive: remember the expectation of $IG\left(\frac{\alpha}{\mu}, \frac{\alpha^2}{\sigma^2}\right)$ is $\frac{\alpha}{\mu}$, which depends only on the threshold $\alpha$ and the expectation $\mu$ of the Astro random walk, &lt;em&gt;not&lt;/em&gt; on higher moments which are crucially underestimated by the Gaussian approximation. On the contrary, quantiles are poorly estimated by the Gaussian model, partly because of the continuous time (quantiles in the discrete time Astro model cannot be less than 1, whereas the inverse Gaussian distribution puts a lot of mass on $[0, 1]$).&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The inverse Gaussian approximation based on continuous time martingale arguments offers an analytical expression that tracks quite accurately the expected flight time of scratching Astro cards. In a sense, this is an empirical manifestation of the ubiquitous property that Gaussian distributions are limits of distributions with finite variance (central limit theorem, Donsker&amp;rsquo;s theorem on random walks&amp;hellip;). This sort of approximation is numerically consistent in expectation. but much less so for other statistics, e.g. the quantiles. Moreover, let&amp;rsquo;s emphasise again the difference in interpretation between mean and median: the expected flight time $\mathbb{E}\left[\tau_{\alpha}\right]\approx 3.17$ is driven upwards by unlikely events of large magnitude; the most likely outcome of scratching Astro cards is to loose 2€ all at once!&lt;/p&gt;
&lt;p&gt;Finally, you may check out the code used for the simulation and figures in this &lt;a href=&#34;https://github.com/sauxpa/astro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What&amp;rsquo;s next?&lt;/h2&gt;
&lt;p&gt;Is this the best we can do? It turns out that the Astro random walk is actually a special case where the martingale arguments developed for the inverse Gaussian approximation do hold in discrete time! We will detail this in the next post and derive in particular the exact expression of $\mathbb{E}\left[\tau_{\alpha}\right]$. Spoiler: the inverse Gaussian approximation was indeed not so bad in expectation!&lt;/p&gt;
&lt;h2 id=&#34;bonus&#34;&gt;Bonus&lt;/h2&gt;
&lt;p&gt;In this first post, we mentioned a subtlety of the sampling mechanism in Astro: bought tickets are effectively removed from the pool of available tickets, and therefore sampling from the Astro distribution should be done without replacement. Intuitively though, this should be negligible since the typical flight time is much smaller than the total amount of tickets (4,500,000). To convince ourselves, we report below figures on mean and variance estimation as well as the distribution of flight time under sampling with no replacement (nr) and visually verify that they are similar to their counterparts with replacement.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /post/astro_2/mu_sigma_astro_nr_vs_gaussian_distributions_hubffa187233cffa206f1dccea7fc6819d_90300_1e474d5bb1441cc5c6d71ae7c9174fcb.png 400w,
               /post/astro_2/mu_sigma_astro_nr_vs_gaussian_distributions_hubffa187233cffa206f1dccea7fc6819d_90300_4b4f99c17fea06e30f82fd4994692b56.png 760w,
               /post/astro_2/mu_sigma_astro_nr_vs_gaussian_distributions_hubffa187233cffa206f1dccea7fc6819d_90300_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/astro_2/mu_sigma_astro_nr_vs_gaussian_distributions_hubffa187233cffa206f1dccea7fc6819d_90300_1e474d5bb1441cc5c6d71ae7c9174fcb.png&#34;
               width=&#34;496&#34;
               height=&#34;712&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /post/astro_2/astro_nr_vs_gaussian_flight_time_distributions_hudfd307929da19b9b32cac73cce6199e6_29446_93cbdc9d6f75da628da6f0744fe323ff.png 400w,
               /post/astro_2/astro_nr_vs_gaussian_flight_time_distributions_hudfd307929da19b9b32cac73cce6199e6_29446_d264fe087a62b2c835893434bd37a64c.png 760w,
               /post/astro_2/astro_nr_vs_gaussian_flight_time_distributions_hudfd307929da19b9b32cac73cce6199e6_29446_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/astro_2/astro_nr_vs_gaussian_flight_time_distributions_hudfd307929da19b9b32cac73cce6199e6_29446_93cbdc9d6f75da628da6f0744fe323ff.png&#34;
               width=&#34;496&#34;
               height=&#34;352&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;Cheers!&lt;/p&gt;
&lt;p&gt;P.S: kudos to &lt;a href=&#34;https://pierrebauvin.netlify.app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pierre Bauvin&lt;/a&gt; for proofreading and for asking that I write it all down in the first place!&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2022-present &lt;a href=&#34;https://sauxpa.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Patrick Saux&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gambler&#39;s ruin in Astro and the accuracy of Gaussian approximation [1]</title>
      <link>https://sauxpa.github.io/post/astro_1/</link>
      <pubDate>Sat, 17 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://sauxpa.github.io/post/astro_1/</guid>
      <description>&lt;p&gt;Recently, I was on a holiday trip with a friend, let&amp;rsquo;s call him Nathan. While not exactly a gambling addict, Nathan likes to buy scratchcards every now and then. One of his favourites is called Astro; most newspapers shops in France sell them for €2 a piece, and there exists 12 different versions, one for each astrological sign. While the sign itself does not affect the odds (at least I don&amp;rsquo;t believe so), it is a brilliant marketing strategy as it encourages you to buy a few more tickets and distribute them to your friends born under a different star than yours. Nathan is a Sagittarius, and that day he acquired a set of tickets, including an extra Aquarius one that he gifted to me. As it turned out, all but one of the set were loosers, the Aquarius one being the sole winner of a modest €6. Emboldened by this unexpected luck, I exchanged my gains for three more tickets to distribute among friends, keeping one Aquarius for myself. Again, all loosers but mine. This time I cashed out and stopped the gambling streak, much to Nathan&amp;rsquo;s dismay. I admit this is not a particularly exciting vacation story, but it got me thinking: what if I did not cashed out? How long could I have last by just reinvesting my gains, flying off Nathan&amp;rsquo;s initial €2 ticket?&lt;/p&gt;
&lt;h2 id=&#34;the-astro-distribution&#34;&gt;The Astro distribution&lt;/h2&gt;
&lt;p&gt;Astro is a fairly transparent game: whoever takes the time to read the back of a ticket can learn all there is to know about the odds and possible profits. On mine, it read that 4,500,000 tickets were issued, among which 661,500 are winners for €2, 476,000 for €4, 150,000 for €6, 101,350 for €10, 45,000 for €20, 415 for €17T00, 8 for €1,000 and finally 3 extra lucky ones for €25,000. The figure below shows this gain distribution, with probabilities displayed in logarithmic scale for readability.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /post/astro_1/astro_distribution_hu57851b484c8509d333975b3cb0092681_18067_3bf1b5b37b9a4f48414549a538a0b505.png 400w,
               /post/astro_1/astro_distribution_hu57851b484c8509d333975b3cb0092681_18067_be83f8c7892889a7459bd64738a3cb47.png 760w,
               /post/astro_1/astro_distribution_hu57851b484c8509d333975b3cb0092681_18067_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/astro_1/astro_distribution_hu57851b484c8509d333975b3cb0092681_18067_3bf1b5b37b9a4f48414549a538a0b505.png&#34;
               width=&#34;495&#34;
               height=&#34;352&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;The expected gain of a ticket chosen uniformly at random (which I assume is how tickets are assigned to vendors) can be easily calculated from the above information and reads $\mathbb{E}[gain] = \sum_{x} x\mathbb{P}(gain=x) = 1.37$. After substracting the €2 cost of a ticket, the expected net payoff of a single Astro ticket is then $-0.63$. This makes perfect sense: the company that issues these tickets makes 63 cents on each, i.e about €2.3m in total.&lt;/p&gt;
&lt;p&gt;In addition, we can also compute the variance with the formula $\sigma^2 = \mathbb{E}[gain^2] - \mathbb{E}[gain]^2$, which gives $\sigma\approx 20.67$.&lt;/p&gt;
&lt;h2 id=&#34;flight-time&#34;&gt;Flight time&lt;/h2&gt;
&lt;p&gt;Going back to our initial question: how long can we defy the odds and keep &amp;ldquo;flying&amp;rdquo; from only the initial investment of €2? Let&amp;rsquo;s introduce some notations. We denote by $\left( g_t \right)_{t\in\mathbb{N}}$ a sequence of i.i.d samples drawn from the Astro distribution and let $\xi_t = 2 - g_t$ be the corresponding net &lt;em&gt;loss&lt;/em&gt;, including the ticket price, and
$$
X_t = \sum_{s=1}^t \xi_s
$$
be the cumulative loss process. The more $X_t$ increases, the more losses the gambler suffers.&lt;/p&gt;
&lt;p&gt;Note that the i.i.d assumption does not exactly hold: once a ticket is bought and scratched, it is removed from the pool of available tickets and cannot be acquired again, i.e. the true sampling is &lt;em&gt;without replacement&lt;/em&gt;. We will see later that this only has a limited impact on the final result, which is quite intuitive: if you draw only a few samples, say a few dozens at most, with replacement, it is quite unlikely that you would sample twice the same among the $N=4,500,000$.&lt;/p&gt;
&lt;p&gt;Our problem of &amp;ldquo;flight time&amp;rdquo; corresponds to studying the following random time:
$$
\tau_{\alpha} = \inf \left\lbrace t\in\mathbb{N}, X_t \geq \alpha \right\rbrace,
$$
which is a stopping time with respect to the natural filtration $\left(\mathcal{F}_t\right)_{t\in\mathbb{N}}$ induced by the process $X$, i.e. $\mathcal{F}_t=\sigma\left( X_s, 0\leq s\leq t\right)$ (this means $\left\lbrace \tau_{\alpha} \geq t\right\rbrace$ is an event in $\mathcal{F}_t$ for all $t\in\mathbb{N}$, i.e. one knows from the information available at time $t$ whether or not the stopping event occured or not). Indeed, reaching the threshold $\alpha=2$ for the cumulative loss $X_t$ is equivalent to reaching €0 wealth for a gambler who starts at €2 and keeps scratching Astro at every round, which we informally defined as the &amp;ldquo;flight time&amp;rdquo; above.&lt;/p&gt;
&lt;h2 id=&#34;the-gaussian-case&#34;&gt;The Gaussian case&lt;/h2&gt;
&lt;p&gt;The reader versed in martingales will almost surely recognise this problem as an instance of gambler&amp;rsquo;s ruin. Indeed, the distribution of $\tau_{\alpha}$ can be fully explicited when the increments $\xi$ are Gaussian, in the limit of continuous time $t\in\mathbb{R}_+$ (instead of $t\in\mathbb{N}$). For completeness, we derive below this standard result using stochastic calculus.&lt;/p&gt;
&lt;p&gt;We define the process $Z$ as
\begin{align*}
&amp;amp;dZ_t = \mu dt + \sigma dW_t\,,\\&lt;br&gt;
&amp;amp;Z_0=0\,,
\end{align*}
where $\mu=0.63$, $\sigma=20.67$ and $W$ is a standard Brownian motion with respect to the filtration $\mathcal{F}$. Intuitively, we retrieve the discrete time model by the correspondence $dt\approx 1$ and $dW_t \approx \xi_t$. The loss process $Z$ can be more directly expressed as
$$
Z_t = \mu t + \sigma W_t\,,
$$
and the flight time in this context naturally becomes $\bar{\tau}_{\alpha} = \inf \left\lbrace t\in\mathbb{R}_+, Z_t \geq \alpha \right\rbrace$.&lt;/p&gt;
&lt;h3 id=&#34;of-martingales-and-stopping-times&#34;&gt;Of martingales and stopping times&lt;/h3&gt;
&lt;p&gt;Studying a stopping time can be quite hard, and coupling it with the right martingale is often the easiest approach. We recall here some elementary results on the interplay between martingales and stopping times. For convenience, we denote by $\mathcal{T}$ either $\mathbb{N}$ or $\mathbb{R}_+$ (the results presented below are essentially unchanged by moving to the continuous or discrete time setting).&lt;/p&gt;
&lt;p&gt;First, a process $\left(M_t\right)_{t\in\mathcal{T}}$ is a martingale with respect to a filtration $\left(\mathcal{F}_t\right)_{t\in\mathcal{T}}$ if for all $t\in\mathcal{T}$, $M_t$ is $\mathcal{F}_t$-adapted, integrable and satisfies the equality
$$
\forall s\in\mathcal{T},\ \mathbb{E}\left[M_{t+s}\mid \mathcal{F}_t\right] = M_t\,.
$$&lt;/p&gt;
&lt;p&gt;Intuitively, this means a martingale represents a &lt;em&gt;fair&lt;/em&gt; game: if $M_t$ represents a fictitious amount of wealth for the player at time $t$, then $M_t$ is fully known from the information available at time $t$ ($\mathcal{F}_t$-adapted) and the estimation of future wealth is exactly $M_t$ (by contrast, a loosing game would satisfy $\mathbb{E}\left[M_{t+s}\mid \mathcal{F}_t\right] \leq M_t$; such processes are called supermartingales). This property is sometimes referred to as martingales being &amp;ldquo;constant in expectation&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The crucial property of martingales for the study of stopping times is called Doob&amp;rsquo;s optional stopping theorem: for any $\mathcal{F}$-stopping time, the &lt;em&gt;stopped&lt;/em&gt; process $\left(M_{t\wedge \tau}\right)_{t\in\mathcal{T}}$ is also a $\mathcal{F}$-martingale, where $t\wedge \tau$ represents the minimum between $t$ and $\tau$. By taking the expectation of the above equality applied to the stopped martingale, we obtain the following identity:
$$
\forall t\in\mathcal{T},\ \mathbb{E}\left[M_{t\wedge \tau}\right] = \mathbb{E}\left[M_0\right]\,.
$$&lt;/p&gt;
&lt;p&gt;Ideally, we would like to have the simpler equality $\mathbb{E}\left[M_{\tau}\right] = \mathbb{E}\left[M_0\right]$ instead, which depending on the exact expression of $M$ can reveal key properties of $\tau$ (expectation or higher moments, density function&amp;hellip;). It is therefore tempting to take the limit $t\rightarrow +\infty$ in the above and hope that one can swap limit and expectation. This holds under a variety of technical assumptions. A simple argument, which is enough for our purpose, is to use the dominated convergence theorem, provided $M_{t\wedge \tau}$ can be bounded by an integrable random variable independent of $t$.&lt;/p&gt;
&lt;h3 id=&#34;building-the-right-martingale&#34;&gt;Building the right martingale&lt;/h3&gt;
&lt;p&gt;For $\lambda\in\mathbb{R}$, we define
$$
M^\lambda_t = \exp\left( \lambda W_t - \frac{\lambda^2}{2}t\right)\,.
$$
It is a standard result in stochastic calculus that $M^\lambda$ defines a $\mathcal{F}$-martingale (it follows from the expression of the moment generating function of the standard Gaussian distribution coupled with the fact that the law of $W_t$ is $\mathcal{N}\left(0, t\right)$). Moreover, it can be rewritten as $M^\lambda_t = \exp\left( \frac{\lambda}{\sigma} Z_t - (\frac{\lambda^2}{2} + \frac{\lambda \mu}{\sigma})t\right)$. The reason why this is the &amp;ldquo;right&amp;rdquo; martingale will become apparent soon: it will help us compute exponential moments of $\bar{\tau}_{\alpha}$, also known as the Laplace transform, which fully characterises its distribution.&lt;/p&gt;
&lt;p&gt;Note that $t\mapsto Z_t$ is (almost-surely) continuous (since $t\mapsto W_t$ is), and therefore $Z_{\bar{\tau}_{\alpha}}=\alpha$ on the event $\left\lbrace \bar{\tau}_{\alpha} &amp;lt; \infty\right\rbrace$. As a consequence, it is straightforward to control $M^{\lambda}_{t\wedge \bar{\tau}_{\alpha}}$ in the following way:
$$
\lvert M^\lambda_{t\wedge \bar{\tau}_{\alpha}} \rvert \leq e^{\frac{\lambda}{\sigma} \max(Z_{t\wedge \bar{\tau}_{\alpha}}, 0) } \leq e^{\frac{\lambda \alpha}{\sigma}}\,,
$$
which allows to use the dominated convergence theorem as discussed above. Therefore, Doob&amp;rsquo;s optional stopping theorem yields:
$$
\mathbb{E}[M^\lambda_{\bar{\tau}_{\alpha}}]=\mathbb{E}[e^{\frac{\lambda \alpha}{\sigma} - (\frac{\lambda^2}{2} + \frac{\lambda \mu}{\sigma})\bar{\tau}_{\alpha}}]=1\,.
$$
After rearranging termes, we obtain the identity
$$
\mathbb{E}[e^{-(\frac{\lambda^2}{2} + \frac{\lambda \mu}{\sigma})\bar{\tau}_{\alpha}}] = e^{-\frac{\lambda \alpha}{\sigma}}\,.
$$
This is almost the Laplace transform of $\bar{\tau}_{\alpha}$, which we define for $\beta&amp;gt;0$ as $\mathbb{E}[e^{-\beta \bar{\tau}_{\alpha}}]$. Since $\bar{\tau}_{\alpha}$ is nonnegative, this Laplace transform fully characterises its distribution. Solving $\beta = \frac{\lambda^2}{2} + \frac{\lambda \mu}{\sigma}$ in terms of $\lambda$ yields $\lambda=-\frac{\mu}{\sigma} + \sqrt{\frac{\mu^2}{\sigma^2} + 2\beta}$ for $\lambda\geq 0$, i.e.
$$
\mathbb{E}[e^{-\beta \bar{\tau}_{\alpha}}] = e^{\frac{\alpha \mu}{\sigma^2}\left(1 - \sqrt{1 + \frac{2\sigma^2 \beta}{\mu^2}}\right)} \,.
$$&lt;/p&gt;
&lt;p&gt;Besides characterising the distribution, the Laplace transform is also useful to compute the moments. Indeed, by differentiating w.r.t. $\beta$ (we differentiate inside the expectation since $e^{-\beta \bar{\tau}}\leq 1$ almost surely), we have that
$$
-\frac{\partial}{\partial \beta} \mathbb{E}[e^{-\beta \bar{\tau}_{\alpha}}]\bigg|_{\beta=0} = \mathbb{E}[\bar{\tau}_{\alpha}] = \frac{\alpha}{\mu}\,.
$$&lt;/p&gt;
&lt;p&gt;After all these calculations, let&amp;rsquo;s take a step back to reflect on what this result means. On average, a gambler reaches the critical threshold $\alpha=2$ after $\frac{\alpha}{\mu}=\frac{2}{0.63}\approx 3.17$ rounds. This is, perhaps quite surprisingly, very intuitive: if you start from €2 and loose on average $63$ cents each time you play, it should take you a bit more than 3 rounds to consume your initial €2. The above shows that this back-of-the-envelope calculation is exact in the continuous time Gaussian case. Moreover, this result is independent of the variance $\sigma^2$.&lt;/p&gt;
&lt;h3 id=&#34;nice-does-this-mean-that-i-can-play-thrice-at-the-cost-of-a-single-ticket-most-of-the-time&#34;&gt;Nice! Does this mean that I can play thrice at the cost of a single ticket most of the time?&lt;/h3&gt;
&lt;p&gt;Not at all! It is important to make the distinction here between &lt;em&gt;mean&lt;/em&gt; and &lt;em&gt;median&lt;/em&gt; flight times. Intuitively, the distribution of flight time $\bar{\tau}_{\alpha}$ should be quite asymmetrical around its mean: events where large gains are made early (e.g. first ticket yields €100) will result in very large flight times but remain quite rare, while events of &amp;ldquo;early crashes&amp;rdquo; (the first few tickets are loosers) are much more frequent. Therefore, the median flight time, i.e. the majority of flight times are below it, should be less than the mean flight time, which is driven upwards by the &amp;ldquo;early luck&amp;rdquo; outliers. This type of distribution is called &lt;em&gt;right-skewed&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;flight-time-distribution-in-the-gaussian-case&#34;&gt;Flight time distribution in the Gaussian case&lt;/h3&gt;
&lt;p&gt;The expression of the Laplace transform $\mathbb{E}[e^{-\beta \bar{\tau}_{\alpha}}]$ reveals that $\bar{\tau}_{\alpha}$ follows an Inverse Gaussian distribution $IG\left(\frac{\alpha}{\mu}, \frac{\alpha^2}{\sigma^2}\right)$, the density of which is given by:
$$
p_{\bar{\tau}_{\alpha}}(t) = \sqrt{\frac{\alpha^2}{2\pi \sigma^2 t^3}} \exp\left(-\frac{\left(\mu t - \alpha\right)^2}{2\sigma^2 t} \right)\,.
$$&lt;/p&gt;
&lt;p&gt;We plot below this density for the Astro parameters ($\mu=0.63, \sigma=20.67, \alpha=2$), as well as the mean, the median, and the 5th, 25th, 75th and 95th percentiles. This distribution appears to be significantly right-skewed: even the 95th percentile is below the mean ($2.14$ versus $3.17$). In other words, according to the Gaussian approximation, it is really unlikely (less than 5% chance) that your flight time will be above 2 rounds, even though the mean flight time is above 3!&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /post/astro_1/ig_distribution_hu56001d7e0d4d1c290b9b1bb2329c0aed_35248_5a997e563d0d77c5a5b9b39911a90a13.png 400w,
               /post/astro_1/ig_distribution_hu56001d7e0d4d1c290b9b1bb2329c0aed_35248_4c8dea43f45b46326e8c0bb308024ad1.png 760w,
               /post/astro_1/ig_distribution_hu56001d7e0d4d1c290b9b1bb2329c0aed_35248_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/astro_1/ig_distribution_hu56001d7e0d4d1c290b9b1bb2329c0aed_35248_5a997e563d0d77c5a5b9b39911a90a13.png&#34;
               width=&#34;637&#34;
               height=&#34;352&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;$\mu$&lt;/th&gt;
&lt;th&gt;$\sigma$&lt;/th&gt;
&lt;th&gt;5th&lt;/th&gt;
&lt;th&gt;25th&lt;/th&gt;
&lt;th&gt;50th&lt;/th&gt;
&lt;th&gt;75th&lt;/th&gt;
&lt;th&gt;95th&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;IG approximation&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;td&gt;20.67&lt;/td&gt;
&lt;td&gt;0.00&lt;/td&gt;
&lt;td&gt;0.01&lt;/td&gt;
&lt;td&gt;0.02&lt;/td&gt;
&lt;td&gt;0.09&lt;/td&gt;
&lt;td&gt;2.14&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What&amp;rsquo;s next?&lt;/h2&gt;
&lt;p&gt;Next time, we will talk about the flight time for the true Astro distribution, rather than under the Gaussian approximation. Deriving a closed-form expression for the distribution of $\tau_{\alpha}$ is more intricate, so simulations will prove handy.&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2022-present &lt;a href=&#34;https://sauxpa.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Patrick Saux&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Markov Epidemic, a stochastic model for disease outbreak</title>
      <link>https://sauxpa.github.io/post/markov_epidemic/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://sauxpa.github.io/post/markov_epidemic/</guid>
      <description>&lt;p&gt;A few years ago, I had the chance to work at ENS Cachan (now ENS Paris-Saclay) on statistical inference of networks based on how information propagates on them. Back then, we used to think in terms of networks of individuals subject to diseases that spread among neighbors, but more realistic application domains were thought to be computer malwares or viral marketing online. Obviously times have changed, and with several weeks of lockdown ahead, I thought about revisiting some of this work in the light of the recent events.&lt;/p&gt;
&lt;p&gt;This article aims to describe a simple model to simulate random disease outbreaks and provide empirical evidence of the role of the network topology. In layman&amp;rsquo;s terms, the sparser the network, such as one describing a population under lockdown, the slower the propagation and the flatter the epidemic peak.&lt;/p&gt;
&lt;p&gt;This post is intended to be fairly non-technical. All experiments shown are based on numerical models and do not claim to accurately represent real-life epidemics. All the code (a small Python library and a few notebooks) and more detailed mathematical explanations can be found &lt;a href=&#34;https://github.com/sauxpa/markov_epidemic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;HERE&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;why-stochastic-models&#34;&gt;Why stochastic models?&lt;/h2&gt;
&lt;p&gt;The use of randomness to simulate complex systems is ubiquitous in mathematics, engineering and sciences in general. The general principle is to build an engine to generate plausible scenarios for the evolution of the system and sample multiple independent realisations to study the average or extreme behaviours.&lt;/p&gt;
&lt;p&gt;Before we move on to such models, let us briefly recall some standard deterministic models in epidemiology. These are called compartmental models, because they model the evolution of a patient as going through successive stages, or compartments. Three popular variants of this idea are :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Susceptible-Infected-Recovered (&lt;strong&gt;SIR&lt;/strong&gt;) : all patients are initially susceptible, then can be infected, and then are removed from the system (either they are fully immune or they died),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Susceptible-Infected-Susceptible (&lt;strong&gt;SIS&lt;/strong&gt;) : same but patients can be infected multiple times,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Susceptible-Exposed-Infectious-Recovered (&lt;strong&gt;SEIR&lt;/strong&gt;) : similar to SIR, with an extra compartment to model incubation time, during which patient are not yet infectious.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Under mean-field assumptions, i.e every individual in a large population shares the same likelihood of being infected by the rest, one can derive a deterministic system of differential equations describing the evolution of the fraction of infected individuals (see for example &lt;a href=&#34;https://ieeexplore.ieee.org/document/130801&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kephart &amp;amp; White&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;While such models are easy to understand and scale very well, they lack any description of the fine structure of the underlying social network. For example, what if the network contains highly social individuals that may act as hubs for the propagation?&lt;/p&gt;
&lt;h2 id=&#34;markov-models&#34;&gt;Markov models&lt;/h2&gt;
&lt;p&gt;A large class of popular stochastic models follow the &lt;strong&gt;Markov property&lt;/strong&gt; : the future evolution depends only on the present state, not the past history. While debatable, it allows for very flexible still numerically tractable models.&lt;/p&gt;
&lt;p&gt;Consider a network &lt;strong&gt;G&lt;/strong&gt;, that is a list of individuals (for instance 1, &amp;hellip;, N) and a list of pairs &lt;strong&gt;(i, j)&lt;/strong&gt; if individuals &lt;strong&gt;i&lt;/strong&gt; and &lt;strong&gt;j&lt;/strong&gt; are neighbors. We will stick to this simple framework, but generalisation to weighted or directed networks is straightforward.&lt;/p&gt;
&lt;p&gt;SIS, SIR and SEIR can be naturally translated to the language of Markov processes by specifying the probability of transition between compartments. As is customary with continuous-time processes, we will actually define the transition rates rather than the probability (the two are related by the fact that transition events follow exponential distributions with parameters equal to the transition rates).&lt;/p&gt;
&lt;p&gt;For any individual, we assume a transition susceptible -&amp;gt; infected (or susceptible -&amp;gt; exposed in SEIR) at rate &lt;em&gt;proportional to the number of infected neighbors&lt;/em&gt;. This is quite natural: the more one is surrounded by infectious people, the riskier (note that here again there is room for more complicated designs by using a nonlinear aggregation function over the neighbors). The other transitions, (I-&amp;gt;S in SIS, I-&amp;gt;R in SIR, E-&amp;gt;I and I-&amp;gt;R in SEIR) are assumed to occur at constant rates (think of it as sick people being put in isolation).&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;h3 id=&#34;sir&#34;&gt;SIR&lt;/h3&gt;
&lt;p&gt;First let us simulate the effect of a severe lockdown in SIR. For that, we compare the evolution of the number of cases for the same epidemic (same transition rates parameters) on two different regular networks :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;sparse&lt;/strong&gt;: every node is connected to 10 other nodes,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;dense&lt;/strong&gt;: every node is connected to 100 other nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the sparse scenario, a typical outbreak has a long plateau and a slow decrease, but a much smaller peak. In the dense case, almost all the population gets infected at a very early stage, creating a sharp peak, followed by a brutal, but slower decrease. The blue curve below is a typical lockdown strategic goal (flat and slow) while the red curve is more inline with a herd immunity viewpoint.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /post/markov_epidemic/sir_epidemic_hue3db7a9b1cec0260dc444a8ba0dbf146_38896_ae24a863317adf89408b7ccb4aa5a8f5.png 400w,
               /post/markov_epidemic/sir_epidemic_hue3db7a9b1cec0260dc444a8ba0dbf146_38896_dc6a7d730c58a00a100e4a979e556065.png 760w,
               /post/markov_epidemic/sir_epidemic_hue3db7a9b1cec0260dc444a8ba0dbf146_38896_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/markov_epidemic/sir_epidemic_hue3db7a9b1cec0260dc444a8ba0dbf146_38896_ae24a863317adf89408b7ccb4aa5a8f5.png&#34;
               width=&#34;760&#34;
               height=&#34;341&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;To be a bit more quantitative about the shape of the curve, one can try to regress a parametric form on it. In all experiments, sparse SIR leads to an approximately normal form while dense SIR is better fitted by a skewed shape such as lognormal. In other words, network sparsification is not &lt;em&gt;just&lt;/em&gt; about flattening the curve, but also controlling the slope.&lt;/p&gt;
&lt;h3 id=&#34;hub-effect&#34;&gt;Hub effect&lt;/h3&gt;
&lt;p&gt;Another empirical effect is the &lt;em&gt;Hub effect&lt;/em&gt;, which is that highly connected nodes are critical for propagation. This typically happens in social networks (a few very popular nodes attract most of the attention) and can be described as &lt;strong&gt;preferential attachment networks&lt;/strong&gt;: starting from a small population of nodes, a larger network is grown by adding follower nodes that connect to the existing ones with probability proportional to the number of already existing neighbors.&lt;/p&gt;
&lt;p&gt;In the case of SIS over a preferential attachment network, we compare two outbreak scenarios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a single patient zero, the one with the most neighbors,&lt;/li&gt;
&lt;li&gt;5 patient zeros, drawn at random (and therefore likely to be follower nodes).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unless a hub is targeted, the epidemic usually dies fast without reaching highly-connected zones, even with more initial patients.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /post/markov_epidemic/hub_vs_random_hu945a41100d4231982b05564604e941aa_44160_4886a62a08d0d13dd2c8a5fa5745b5ec.png 400w,
               /post/markov_epidemic/hub_vs_random_hu945a41100d4231982b05564604e941aa_44160_aa44372e8f974abf4453a85379d5865c.png 760w,
               /post/markov_epidemic/hub_vs_random_hu945a41100d4231982b05564604e941aa_44160_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/markov_epidemic/hub_vs_random_hu945a41100d4231982b05564604e941aa_44160_4886a62a08d0d13dd2c8a5fa5745b5ec.png&#34;
               width=&#34;760&#34;
               height=&#34;288&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h3 id=&#34;comparison-with-deterministic-models&#34;&gt;Comparison with deterministic models&lt;/h3&gt;
&lt;p&gt;As argued above, deterministic models à la Kephart &amp;amp; White can be seen as scaling limits of Markov models on regular networks, i.e when the local structure of the network is the same everywhere one looks.&lt;/p&gt;
&lt;p&gt;This is indeed empirically validated, see for example in the case of SIR and SIS below.&lt;/p&gt;














&lt;figure  id=&#34;figure-sir&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;SIR&#34; srcset=&#34;
               /post/markov_epidemic/sir_deterministic_hu8699e696a741ec40be17a8e3b92182c8_38195_5e9b1458f1c5822dfd8a7d61e318fddc.png 400w,
               /post/markov_epidemic/sir_deterministic_hu8699e696a741ec40be17a8e3b92182c8_38195_84b327bf2a0659118ee0c9659c368560.png 760w,
               /post/markov_epidemic/sir_deterministic_hu8699e696a741ec40be17a8e3b92182c8_38195_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/markov_epidemic/sir_deterministic_hu8699e696a741ec40be17a8e3b92182c8_38195_5e9b1458f1c5822dfd8a7d61e318fddc.png&#34;
               width=&#34;760&#34;
               height=&#34;406&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      SIR
    &lt;/figcaption&gt;&lt;/figure&gt;














&lt;figure  id=&#34;figure-sis&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;SIS&#34; srcset=&#34;
               /post/markov_epidemic/sis_deterministic_hu6f05b0ba611a9dd140145b059187cacd_41995_30a5f5ab0fcfb6caf0bda833bde40b13.png 400w,
               /post/markov_epidemic/sis_deterministic_hu6f05b0ba611a9dd140145b059187cacd_41995_b0973078d91d62c3ab05a671e6a6a5f0.png 760w,
               /post/markov_epidemic/sis_deterministic_hu6f05b0ba611a9dd140145b059187cacd_41995_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/markov_epidemic/sis_deterministic_hu6f05b0ba611a9dd140145b059187cacd_41995_30a5f5ab0fcfb6caf0bda833bde40b13.png&#34;
               width=&#34;760&#34;
               height=&#34;406&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      SIS
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;h3 id=&#34;comparison-with-real-data-as-of-april-2020&#34;&gt;Comparison with real data (as of April 2020)&lt;/h3&gt;
&lt;p&gt;This is still a work-in-progress, made practically difficult by the number and quality of available data. One interesting direction would be to fit one of these models (for example SEIR) to the curve of tested cases of COVID-19 (which is typically noisy and subject to bias, including periodic bias, for instance during the week-ends when medical staff are not tested).&lt;/p&gt;
&lt;p&gt;To be clear, calibrating the model here means tuning the transition rates for S-&amp;gt;E, E-&amp;gt;I and I-&amp;gt;R such that the average curve over multiple epidemic scenarios resembles the most the collected data (for the example in the least squares sense).&lt;/p&gt;
&lt;p&gt;From there, one could investigate different types of representative networks to determine the ones that reproduce the best the observed trends, and use this as a starting point for further predictions (epidemic evolution, impact of lockdown exit&amp;hellip;).&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /post/markov_epidemic/seir_france_hu6a4ec7a8907334a61116910f29942a3f_41715_e6bac73770a657dca40d1507ed013e05.png 400w,
               /post/markov_epidemic/seir_france_hu6a4ec7a8907334a61116910f29942a3f_41715_80e12fbf403b1b95dbff3500415850a4.png 760w,
               /post/markov_epidemic/seir_france_hu6a4ec7a8907334a61116910f29942a3f_41715_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/markov_epidemic/seir_france_hu6a4ec7a8907334a61116910f29942a3f_41715_e6bac73770a657dca40d1507ed013e05.png&#34;
               width=&#34;760&#34;
               height=&#34;341&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;h2 id=&#34;going-further&#34;&gt;Going further&lt;/h2&gt;
&lt;p&gt;A very attractive aspect of stochastic simulation is the ability to perform numerical experiments. However, the topic of random processes over networks has been studied a lot on the theoretical side as well. A few selected references include :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nas.ewi.tudelft.nl/people/Piet/papers/IEEEToN_virusspread.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Virus Spreads in Networks&lt;/a&gt; by Van Mieghem et al., which formalises the definition of epidemic as a Markov process on the configuration space,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://people.maths.bris.ac.uk/~maajg/infocom-worm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Effect of Network Topology on the Spread of Epidemics&lt;/a&gt; by Massoulié et al., which exhibits a phase transition between short-lived and long-lived epidemics in terms of the transition rates and the adjacency spectral radius of the network,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1004.0060.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Got the Flu (or Mumps)? Check the Eigenvalue!&lt;/a&gt; in the same spirit, which relates spectral properties of the network to critical epidemic regimes in different models.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also check out this &lt;a href=&#34;https://github.com/sauxpa/markov_epidemic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;repo&lt;/a&gt; for more implementation details, including a Bokeh app to dynamically play with different network and epidemic parameters.&lt;/p&gt;














&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;
        &lt;img alt=&#34;&#34; srcset=&#34;
               /post/markov_epidemic/bokeh_example_hue8dbace8aa1f04c35cf3cc25e25b08c3_325440_0346892dd2e251724b8d6fa462f712ff.png 400w,
               /post/markov_epidemic/bokeh_example_hue8dbace8aa1f04c35cf3cc25e25b08c3_325440_4980a817c69a8efc1fc0921f86bcb581.png 760w,
               /post/markov_epidemic/bokeh_example_hue8dbace8aa1f04c35cf3cc25e25b08c3_325440_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://sauxpa.github.io/post/markov_epidemic/bokeh_example_hue8dbace8aa1f04c35cf3cc25e25b08c3_325440_0346892dd2e251724b8d6fa462f712ff.png&#34;
               width=&#34;760&#34;
               height=&#34;525&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;p&gt;Enjoy!&lt;/p&gt;
&lt;h2 id=&#34;license&#34;&gt;License&lt;/h2&gt;
&lt;p&gt;Copyright 2020-present &lt;a href=&#34;https://sauxpa.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Patrick Saux&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
